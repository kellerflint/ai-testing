{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from gradientai import Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GRADIENT_ACCESS_TOKEN'] = \"\"\n",
    "os.environ['GRADIENT_WORKSPACE_ID'] = \"\"\n",
    "\n",
    "role_play_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_file = \"mega_output.csv\"\n",
    "df = pd.read_csv(path_to_csv_file, header=None, names=[\"prompt\", \"response\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range (df.shape[0]):\n",
    "    prompt = df['prompt'].iloc[i]\n",
    "    response = df['response'].iloc[i]\n",
    "\n",
    "    start_str = f\"<s>### Instruction:\\n{role_play_prompt}\\n\\n###Input:\\n\"\n",
    "    prompt = prompt.replace('\"','\\\\\"')\n",
    "    mid_str = '''\\n\\n### Response:\\n'''\n",
    "    response = response.replace('\"','\\\\\"')\n",
    "    end_str = '''</s>'''\n",
    "    total_line = start_str + prompt + mid_str + response + end_str\n",
    "\n",
    "    # each line of training data is a simple object: 'inputs' and actual training string\n",
    "    obj = {\n",
    "        \"inputs\" : total_line\n",
    "    }\n",
    "    lines.append(obj)\n",
    "\n",
    "# print(total_line) # comment in to see how the formatted lines look\n",
    "# these lines could also be written to a jsonl file for use\n",
    "# with the command line interface\n",
    "print(f\"Generated {len(lines)} lines to fine-tune\")\n",
    "print(f\"Example training line: {lines[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning model adapter\n",
      "Created model with ID 57d61074-0e10-486a-b355-718bf5c565be_model_adapter\n",
      "Fine-tuning chunk 0 of 16\n",
      "Fine-tuning chunk 1 of 16\n",
      "Fine-tuning chunk 2 of 16\n",
      "Fine-tuning chunk 3 of 16\n",
      "Fine-tuning chunk 4 of 16\n",
      "Fine-tuning chunk 5 of 16\n",
      "Fine-tuning chunk 6 of 16\n",
      "Fine-tuning chunk 7 of 16\n",
      "Fine-tuning chunk 8 of 16\n",
      "Fine-tuning chunk 9 of 16\n",
      "Fine-tuning chunk 10 of 16\n",
      "*** Error processing chunk 10: (422) Reason: unknown\n",
      "Fine-tuning chunk 11 of 16\n",
      "Fine-tuning chunk 12 of 16\n",
      "Fine-tuning chunk 13 of 16\n",
      "*** Error processing chunk 13: (422) Reason: unknown\n",
      "Fine-tuning chunk 14 of 16\n",
      "Fine-tuning chunk 15 of 16\n",
      "Fine-tuning chunk 16 of 16\n"
     ]
    }
   ],
   "source": [
    "# split up the lines into manageable chunks\n",
    "lines_per_chunk = 20\n",
    "all_chunks = []\n",
    "for line in lines:\n",
    "  if len(all_chunks) == 0 or len(all_chunks[-1]) == lines_per_chunk:\n",
    "    all_chunks.append([])\n",
    "  all_chunks[-1].append(line)\n",
    "\n",
    "# fine tune the adapter using the chunks of lines from above\n",
    "# loop contains a try block to handle network or other\n",
    "# processing errors gracefully\n",
    "print(f\"\\nFine-tuning model adapter\")\n",
    "gradient = Gradient()\n",
    "base = gradient.get_base_model(base_model_slug=\"nous-hermes2\")\n",
    "my_adapter = base.create_model_adapter(name=\"megabot\")\n",
    "print(f\"Created model with ID {my_adapter.id}\")\n",
    "for i in range(len(all_chunks)):\n",
    "  try:\n",
    "    print(f\"Fine-tuning chunk {i} of {len(all_chunks) - 1}\")\n",
    "    my_adapter.fine_tune(samples=all_chunks[i])\n",
    "  except Exception as error:\n",
    "    try:\n",
    "      error_pieces = str(error).split(\"\\n\")\n",
    "      if len(error_pieces) > 1:\n",
    "        print(f\"*** Error processing chunk {i}: {error_pieces[0]} {error_pieces[1]}\")\n",
    "      else:\n",
    "        print(f\"*** Unknown error on chunk {i}: {error}\")\n",
    "    except KeyboardInterrupt:\n",
    "      break\n",
    "    except Exception as inner_error:\n",
    "      print(inner_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megabot: 57d61074-0e10-486a-b355-718bf5c565be_model_adapter\n"
     ]
    }
   ],
   "source": [
    "# if your colab instance gets deleted, you can run this to get the model names\n",
    "gradient = Gradient()\n",
    "# if necessary, go back and find your previously created models and their IDs\n",
    "old_models = gradient.list_models(only_base=False)\n",
    "for model in old_models:\n",
    "  if hasattr(model, \"name\"):\n",
    "    print(f\"{model.name}: {model.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Um\"\n",
    "templated_query = f\"<s>### Instruction:\\n{role_play_prompt}\\n\\n###Input:\\n{query}\\n\\n### Response:\\n\"\n",
    "response = my_adapter.complete(query=templated_query, max_generated_token_count=500)\n",
    "print(f\"> {query}\\n> {response.generated_output}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
